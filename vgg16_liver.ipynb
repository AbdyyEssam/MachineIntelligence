{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65535915",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a74e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hunda\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff31b5b",
   "metadata": {},
   "source": [
    "# data reading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d018172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = Path('C:/Users/hunda/OneDrive/Desktop/liver1')\n",
    "#image_dir2 = Path('C:/Users/hunda/OneDrive/Desktop/heart-test')\n",
    "#data_dir = pathlib.Path(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff55dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15d5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1419 images belonging to 2 classes.\n",
      "Found 353 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "trdata = ImageDataGenerator(\n",
    "    validation_split=0.2,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "traindata = trdata.flow_from_directory(directory=image_dir,target_size=(224,224),\n",
    "    batch_size = 16, subset=\"training\")\n",
    "valdata = trdata.flow_from_directory(directory=image_dir,target_size=(224,224),\n",
    "    batch_size = 16,subset=\"validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c5a66",
   "metadata": {},
   "source": [
    "# implementing vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e8900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape= [224,224]+[3] ,include_top=False, weights='imagenet')\n",
    "vgg.trainable= False\n",
    "#DEFINING THE VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c4f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layers in (vgg.layers)[:14]:\n",
    "#     print(layers)\n",
    "#     layers.trainable = False\n",
    "#freezing some layers for tunning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f4aca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bedd682",
   "metadata": {},
   "source": [
    "# fittting the model and defining the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9a3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg.trainable = False ## Not trainable weights\n",
    "# from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "# train_ds = preprocess_input(train_ds) \n",
    "# val_ds = preprocess_input(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b3a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "# normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "# normalized_ds2 = val_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06cd7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# # flatten_layer = layers.Flatten()\n",
    "# # dense_layer_1 = layers.Dense(50, activation='relu')\n",
    "# # dense_layer_2 = layers.Dense(20, activation='relu')\n",
    "# # prediction_layer = layers.Dense(2, activation='softmax')\n",
    "# num_classes = 2\n",
    "# flatten_layer = layers.Flatten()\n",
    "# model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Rescaling(1./255),flatten_layer,\n",
    "#   tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Flatten(),\n",
    "#   #tf.keras.layers.Dense(16, activation='softmax'),\n",
    "#   tf.keras.layers.Dense(num_classes)\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "# # model = models.Sequential([\n",
    "# #     vgg,\n",
    "# #     flatten_layer,\n",
    "# #     dense_layer_1,\n",
    "# #     dense_layer_2,\n",
    "# #     prediction_layer\n",
    "# # ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30c48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(64, activation='relu')\n",
    "dense_layer_2 = layers.Dense(32, activation='relu')\n",
    "dense_layer_3 = layers.Dense(16, activation='relu')\n",
    "prediction_layer = layers.Dense(2, activation='softmax')\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    vgg,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    dense_layer_3,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638bd8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x000001C304C8E7F0>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842a9cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.7984\n",
      "Epoch 1: val_loss improved from inf to 0.38780, saving model to C:/Users/hunda/OneDrive/Desktop\\liver-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/hunda/OneDrive/Desktop\\liver-test\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/hunda/OneDrive/Desktop\\liver-test\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "89/89 [==============================] - 220s 2s/step - loss: 0.4118 - accuracy: 0.7984 - val_loss: 0.3878 - val_accuracy: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c304c9eac0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "lr = 1e-4\n",
    "optimizer = Adam(lr)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "\n",
    "filepath='C:/Users/hunda/OneDrive/Desktop/liver-test'\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpoint, es]\n",
    "\n",
    "model.fit(traindata, validation_data=valdata, epochs=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e8449",
   "metadata": {},
   "source": [
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cd5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test loss: 0.39421\n",
      "test accuracy: 85.84%\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(valdata, verbose = 0)\n",
    "print(\"  test loss: {:.5f}\".format(results[0]))\n",
    "print(\"test accuracy: {:.2f}%\".format(results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f1208",
   "metadata": {},
   "source": [
    "# plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f00747b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a81f81b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\hunda\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\hunda\\anaconda3\\lib\\site-packages (from pydot) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83129718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
